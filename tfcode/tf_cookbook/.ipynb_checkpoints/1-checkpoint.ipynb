{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在tensorflow中创建张量tensor\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function zeros in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "zeros(shape, dtype=tf.float32, name=None)\n",
      "    Creates a tensor with all elements set to zero.\n",
      "    \n",
      "    This operation returns a tensor of type `dtype` with shape `shape` and\n",
      "    all elements set to zero.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    tf.zeros([3, 4], tf.int32)  # [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      shape: A list of integers, a tuple of integers, or a 1-D `Tensor` of type\n",
      "        `int32`.\n",
      "      dtype: The type of an element in the resulting `Tensor`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with all elements set to zero.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建fixed张量\n",
    "help(tf.zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "zero_tsr = tf.zeros(shape=[3, 4], dtype=tf.float32, name='zeros')\n",
    "print(zero_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(zero_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones:0\", shape=(3, 4), dtype=float32)\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_tsr = tf.ones(shape=[3, 4], dtype=tf.float32, name='ones')\n",
    "print(one_tsr)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(one_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fill in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "fill(dims, value, name=None)\n",
      "    Creates a tensor filled with a scalar value.\n",
      "    \n",
      "    This operation creates a tensor of shape `dims` and fills it with `value`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```\n",
      "    # Output tensor has shape [2, 3].\n",
      "    fill([2, 3], 9) ==> [[9, 9, 9]\n",
      "                         [9, 9, 9]]\n",
      "    ```\n",
      "    \n",
      "    `tf.fill` differs from `tf.constant` in a few ways:\n",
      "    \n",
      "    *   `tf.fill` only supports scalar contents, whereas `tf.constant` supports\n",
      "        Tensor values.\n",
      "    *   `tf.fill` creates an Op in the computation graph that constructs the actual\n",
      "        Tensor value at runtime. This is in contrast to `tf.constant` which embeds\n",
      "        the entire Tensor into the graph with a `Const` node.\n",
      "    *   Because `tf.fill` evaluates at graph runtime, it supports dynamic shapes\n",
      "        based on other runtime Tensors, unlike `tf.constant`.\n",
      "    \n",
      "    Args:\n",
      "      dims: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        1-D. Represents the shape of the output tensor.\n",
      "      value: A `Tensor`. 0-D (scalar). Value to fill the returned tensor.\n",
      "    \n",
      "        @compatibility(numpy)\n",
      "        Equivalent to np.full\n",
      "        @end_compatibility\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `value`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fill_1:0\", shape=(3, 4), dtype=int32)\n",
      "[[42 42 42 42]\n",
      " [42 42 42 42]\n",
      " [42 42 42 42]]\n"
     ]
    }
   ],
   "source": [
    "fill_tsr = tf.fill(dims=[3, 4], value=42, name='fill')\n",
    "print(fill_tsr)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(fill_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"constant:0\", shape=(3,), dtype=int32)\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "constant_tsr = tf.constant(value=[1, 2, 3], name='constant')\n",
    "print(constant_tsr)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(constant_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_like:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 根据另外一个张量的形状定义一个新的张量\n",
    "zeros_similar = tf.zeros_like(constant_tsr)\n",
    "print(zeros_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(zeros_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones_like:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ones_similar = tf.ones_like(constant_tsr)\n",
    "print(ones_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(ones_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lin_space in module tensorflow.python.ops.gen_math_ops:\n",
      "\n",
      "lin_space(start, stop, num, name=None)\n",
      "    Generates values in an interval.\n",
      "    \n",
      "    A sequence of `num` evenly-spaced values are generated beginning at `start`.\n",
      "    If `num > 1`, the values in the sequence increase by `stop - start / num - 1`,\n",
      "    so that the last one is exactly `stop`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```\n",
      "    tf.linspace(10.0, 12.0, 3, name=\"linspace\") => [ 10.0  11.0  12.0]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      start: A `Tensor`. Must be one of the following types: `bfloat16`, `float32`, `float64`.\n",
      "        0-D tensor. First entry in the range.\n",
      "      stop: A `Tensor`. Must have the same type as `start`.\n",
      "        0-D tensor. Last entry in the range.\n",
      "      num: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        0-D tensor. Number of values to generate.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `start`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 张量序列\n",
    "help(tf.linspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"linspace_1:0\", shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "linear_tsr = tf.linspace(start=0., stop=3., num=3, name='linspace')\n",
    "print(linear_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  1.5 3. ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(linear_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "linear_tsr = tf.linspace(start=0., stop=1., num=3, name='linspace')\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(linear_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function range in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "range(start, limit=None, delta=1, dtype=None, name='range')\n",
      "    Creates a sequence of numbers.\n",
      "    \n",
      "    Creates a sequence of numbers that begins at `start` and extends by\n",
      "    increments of `delta` up to but not including `limit`.\n",
      "    \n",
      "    The dtype of the resulting tensor is inferred from the inputs unless\n",
      "    it is provided explicitly.\n",
      "    \n",
      "    Like the Python builtin `range`, `start` defaults to 0, so that\n",
      "    `range(n) = range(0, n)`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    start = 3\n",
      "    limit = 18\n",
      "    delta = 3\n",
      "    tf.range(start, limit, delta)  # [3, 6, 9, 12, 15]\n",
      "    \n",
      "    start = 3\n",
      "    limit = 1\n",
      "    delta = -0.5\n",
      "    tf.range(start, limit, delta)  # [3, 2.5, 2, 1.5]\n",
      "    \n",
      "    limit = 5\n",
      "    tf.range(limit)  # [0, 1, 2, 3, 4]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      start: A 0-D `Tensor` (scalar). Acts as first entry in the range if\n",
      "        `limit` is not None; otherwise, acts as range limit and first entry\n",
      "        defaults to 0.\n",
      "      limit: A 0-D `Tensor` (scalar). Upper limit of sequence,\n",
      "        exclusive. If None, defaults to the value of `start` while the first\n",
      "        entry of the range defaults to 0.\n",
      "      delta: A 0-D `Tensor` (scalar). Number that increments\n",
      "        `start`. Defaults to 1.\n",
      "      dtype: The type of the elements of the resulting tensor.\n",
      "      name: A name for the operation. Defaults to \"range\".\n",
      "    \n",
      "    Returns:\n",
      "      An 1-D `Tensor` of type `dtype`.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    Equivalent to np.arange\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 还可以使用tf.range产生序列，只不过不包含最大值\n",
    "help(tf.range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"range:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "integer_seq_tsr = tf.range(start=6, limit=15, delta=3)\n",
    "print(integer_seq_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  9 12]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(integer_seq_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生随机张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function random_uniform in module tensorflow.python.ops.random_ops:\n",
      "\n",
      "random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)\n",
      "    Outputs random values from a uniform distribution.\n",
      "    \n",
      "    The generated values follow a uniform distribution in the range\n",
      "    `[minval, maxval)`. The lower bound `minval` is included in the range, while\n",
      "    the upper bound `maxval` is excluded.\n",
      "    \n",
      "    For floats, the default range is `[0, 1)`.  For ints, at least `maxval` must\n",
      "    be specified explicitly.\n",
      "    \n",
      "    In the integer case, the random integers are slightly biased unless\n",
      "    `maxval - minval` is an exact power of two.  The bias is small for values of\n",
      "    `maxval - minval` significantly smaller than the range of the output (either\n",
      "    `2**32` or `2**64`).\n",
      "    \n",
      "    Args:\n",
      "      shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
      "      minval: A 0-D Tensor or Python value of type `dtype`. The lower bound on the\n",
      "        range of random values to generate.  Defaults to 0.\n",
      "      maxval: A 0-D Tensor or Python value of type `dtype`. The upper bound on\n",
      "        the range of random values to generate.  Defaults to 1 if `dtype` is\n",
      "        floating point.\n",
      "      dtype: The type of the output: `float16`, `float32`, `float64`, `int32`,\n",
      "        or `int64`.\n",
      "      seed: A Python integer. Used to create a random seed for the distribution.\n",
      "        See `tf.set_random_seed`\n",
      "        for behavior.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A tensor of the specified shape filled with random uniform values.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `dtype` is integral and `maxval` is not specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 均匀分布\n",
    "help(tf.random_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"uniform_1:0\", shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "randunif_tsr = tf.random_uniform(shape=[3, 4], minval=0, maxval=1, dtype=tf.float32, seed=1234, name='uniform')\n",
    "print(randunif_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.848307   0.32357132 0.3067001  0.06969976]\n",
      " [0.9138565  0.17047906 0.2833712  0.35627055]\n",
      " [0.54155624 0.07525682 0.07449007 0.86595404]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(randunif_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"normal_1:0\", shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 正态分布\n",
    "randnorm_tsr = tf.random_normal(shape=[3, 4], mean=0., stddev=1., name='normal')\n",
    "print(randnorm_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8070657  -0.5877109  -0.07889919  0.5804393 ]\n",
      " [ 0.81997955 -0.43158466  0.13511424  1.2051883 ]\n",
      " [-1.2998413   0.56384933 -0.25600737 -0.29819706]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(randnorm_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truncated_normal:0\", shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 截断正态分布\n",
    "truncnorm_tsr = tf.truncated_normal(shape=[3, 4], mean=0., stddev=1., dtype=tf.float32, name='truncated_normal')\n",
    "print(truncnorm_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.202312    0.0118741   0.8655835   1.7842199 ]\n",
      " [-1.5990945   0.03537165 -0.49096245  0.4029202 ]\n",
      " [-0.4582348  -0.04305958 -0.92269886 -0.5074301 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(truncnorm_tsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function random_shuffle in module tensorflow.python.ops.random_ops:\n",
      "\n",
      "random_shuffle(value, seed=None, name=None)\n",
      "    Randomly shuffles a tensor along its first dimension.\n",
      "    \n",
      "    The tensor is shuffled along dimension 0, such that each `value[j]` is mapped\n",
      "    to one and only one `output[i]`. For example, a mapping that might occur for a\n",
      "    3x2 tensor is:\n",
      "    \n",
      "    ```python\n",
      "    [[1, 2],       [[5, 6],\n",
      "     [3, 4],  ==>   [1, 2],\n",
      "     [5, 6]]        [3, 4]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      value: A Tensor to be shuffled.\n",
      "      seed: A Python integer. Used to create a random seed for the distribution.\n",
      "        See\n",
      "        `tf.set_random_seed`\n",
      "        for behavior.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A tensor of same shape and type as `value`, shuffled along its first\n",
      "      dimension.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数组的随机操作\n",
    "help(tf.random_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "----------------------------------\n",
      "[[ 1  2  3]\n",
      " [10 11 12]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]]\n"
     ]
    }
   ],
   "source": [
    "# 对张量进行随机打乱\n",
    "input_tsr = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], name='input_tsr')\n",
    "shuffled_output = tf.random_shuffle(input_tsr, seed=123, name='shuffled_out')\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(input_tsr))\n",
    "    print('----------------------------------')\n",
    "    print(sess.run(shuffled_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function random_crop in module tensorflow.python.ops.random_ops:\n",
      "\n",
      "random_crop(value, size, seed=None, name=None)\n",
      "    Randomly crops a tensor to a given size.\n",
      "    \n",
      "    Slices a shape `size` portion out of `value` at a uniformly chosen offset.\n",
      "    Requires `value.shape >= size`.\n",
      "    \n",
      "    If a dimension should not be cropped, pass the full size of that dimension.\n",
      "    For example, RGB images can be cropped with\n",
      "    `size = [crop_height, crop_width, 3]`.\n",
      "    \n",
      "    Args:\n",
      "      value: Input tensor to crop.\n",
      "      size: 1-D tensor with size the rank of `value`.\n",
      "      seed: Python integer. Used to create a random seed. See\n",
      "        `tf.set_random_seed`\n",
      "        for behavior.\n",
      "      name: A name for this operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A cropped tensor of the same rank as `value` and shape `size`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.random_crop\n",
    "help(tf.random_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "cropped_output = tf.random_crop(input_tsr, [2, 2], seed=123, name='cropped')\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cropped_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  9.182167   46.894775   20.655664 ]\n",
      "  [157.99515   159.1455     89.20461  ]\n",
      "  [ 14.964607   65.67442     6.6673546]\n",
      "  ...\n",
      "  [ 36.575516   95.956535   92.02345  ]\n",
      "  [128.10144   199.10323   160.42949  ]\n",
      "  [ 27.813648  203.65613    51.409416 ]]\n",
      "\n",
      " [[ 52.785767   42.098557  143.28087  ]\n",
      "  [ 33.264168   87.3099     74.97525  ]\n",
      "  [ 74.83792   220.81854   182.9596   ]\n",
      "  ...\n",
      "  [ 98.84935   132.27719   123.49615  ]\n",
      "  [ 58.749928  222.6389    192.07297  ]\n",
      "  [ 15.558073   31.24229   156.47476  ]]\n",
      "\n",
      " [[201.3331     17.34378   100.280396 ]\n",
      "  [168.04855    82.54412   209.55908  ]\n",
      "  [ 45.61901   113.49872    96.46396  ]\n",
      "  ...\n",
      "  [108.37125   138.01735   128.59514  ]\n",
      "  [113.0075     13.614544   25.292454 ]\n",
      "  [221.6439     95.956535  159.66898  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[189.47101    56.819138   88.9457   ]\n",
      "  [ 81.276     169.36674   156.9615   ]\n",
      "  [121.28053    18.704037  147.54199  ]\n",
      "  ...\n",
      "  [ 60.801495  135.21558   195.62071  ]\n",
      "  [ 29.85929   157.61977   196.84941  ]\n",
      "  [ 83.1043     85.138596   46.291897 ]]\n",
      "\n",
      " [[133.0105     41.404804   67.99252  ]\n",
      "  [220.8716     49.082626   92.363716 ]\n",
      "  [177.28224   221.35727   116.26478  ]\n",
      "  ...\n",
      "  [ 32.190826   31.709799   35.910355 ]\n",
      "  [171.4273    168.796      40.73181  ]\n",
      "  [ 28.759422  217.40553    61.243816 ]]\n",
      "\n",
      " [[160.03934   157.26724   124.9219   ]\n",
      "  [141.1407     85.18036    59.695457 ]\n",
      "  [124.78487   100.495155  187.17865  ]\n",
      "  ...\n",
      "  [179.06374    92.584816  107.23343  ]\n",
      "  [  4.7773094  78.56909   104.58724  ]\n",
      "  [122.77455   124.04361   117.46262  ]]]\n",
      "------------------------------\n",
      "[[[134.90962    171.32874     54.469925  ]\n",
      "  [ 83.960495   126.49059    148.83476   ]\n",
      "  [212.55756     13.745624     7.9702377 ]\n",
      "  [ 66.378716   134.1704     201.21803   ]\n",
      "  [ 40.924664    31.337294   101.74775   ]\n",
      "  [126.5926     129.84059    184.73497   ]\n",
      "  [  8.895519     4.396409   209.43492   ]\n",
      "  [140.44333    208.93784    114.933754  ]\n",
      "  [186.39403     76.38191     42.72091   ]\n",
      "  [ 25.205685    55.63859    125.43286   ]\n",
      "  [205.18857    146.21703     29.005087  ]\n",
      "  [ 79.50346     25.272713     3.4102142 ]]\n",
      "\n",
      " [[ 54.923782   131.30159     91.876015  ]\n",
      "  [206.6609     206.76366    114.36435   ]\n",
      "  [ 52.839947   173.32423    116.27719   ]\n",
      "  [183.29602    193.48654     21.754217  ]\n",
      "  [224.59361    107.33956     73.81455   ]\n",
      "  [125.00371    217.53702    221.60837   ]\n",
      "  [ 90.07807    194.15825    224.73425   ]\n",
      "  [ 86.065384   138.3488      23.641714  ]\n",
      "  [ 50.581688    56.69302    149.15059   ]\n",
      "  [ 95.656395    94.28018     26.291763  ]\n",
      "  [ 27.21415    142.92764     80.91954   ]\n",
      "  [  0.9325236  208.34407    115.72914   ]]\n",
      "\n",
      " [[134.56552    103.73194    202.93394   ]\n",
      "  [177.10828     66.271324   113.72381   ]\n",
      "  [ 51.785168    54.65202     29.446041  ]\n",
      "  [102.710686    96.24313     95.439514  ]\n",
      "  [ 81.41296      6.9019136   71.09584   ]\n",
      "  [172.63025    122.34835     61.22778   ]\n",
      "  [  0.80337524  52.077423   190.84462   ]\n",
      "  [  7.697806    94.75713    151.892     ]\n",
      "  [ 42.67314     16.294743   219.20511   ]\n",
      "  [142.54976    179.92668    123.19863   ]\n",
      "  [  3.5856843   75.52932     94.04809   ]\n",
      "  [ 37.13508     38.017765   153.52664   ]]\n",
      "\n",
      " [[118.59701     41.49423     68.48063   ]\n",
      "  [ 89.66088    211.9313      94.83776   ]\n",
      "  [ 51.755745   201.0137       3.0233054 ]\n",
      "  [ 58.42527     78.75856    113.73173   ]\n",
      "  [177.04517    121.3893     124.920074  ]\n",
      "  [  4.9253407   44.486557    42.58857   ]\n",
      "  [122.65409    198.07834     28.934141  ]\n",
      "  [120.47176     65.861694   147.22855   ]\n",
      "  [ 38.37552    127.13609    184.91774   ]\n",
      "  [193.4209     113.72577    182.95764   ]\n",
      "  [ 74.13577     27.819094     5.4972677 ]\n",
      "  [ 94.06212    145.42706     77.09261   ]]\n",
      "\n",
      " [[126.877686   187.79594    126.53629   ]\n",
      "  [165.77078     22.214163   126.425064  ]\n",
      "  [174.88252    165.33273      4.614955  ]\n",
      "  [ 14.548865    13.70185     10.477003  ]\n",
      "  [215.46585     22.275639   195.30283   ]\n",
      "  [ 21.172232   216.76541     34.086372  ]\n",
      "  [192.12854     71.0957      26.428583  ]\n",
      "  [ 36.62959     55.672306    23.083601  ]\n",
      "  [145.65486     97.074455   180.72362   ]\n",
      "  [ 43.273525    84.04313    185.53168   ]\n",
      "  [162.7353     221.04543      4.946771  ]\n",
      "  [ 23.511627   164.36049      3.4591646 ]]\n",
      "\n",
      " [[210.17554    118.50887    142.5159    ]\n",
      "  [ 10.722425   173.89864    183.44815   ]\n",
      "  [212.6723      98.70561    194.92221   ]\n",
      "  [ 59.137638    13.792912   215.8612    ]\n",
      "  [ 48.382168   106.96703      4.5761437 ]\n",
      "  [208.09093     68.86977    205.6705    ]\n",
      "  [  6.792292   164.76459     66.7346    ]\n",
      "  [ 73.91771    143.55641      5.705434  ]\n",
      "  [102.537254   132.29033    121.450264  ]\n",
      "  [224.05922    223.38652    112.34979   ]\n",
      "  [114.84921    147.24281    183.44      ]\n",
      "  [ 23.67315      4.34365    212.95581   ]]\n",
      "\n",
      " [[173.72134    143.04805    173.25037   ]\n",
      "  [ 93.45734    139.87418    140.90398   ]\n",
      "  [167.59286    102.10909     88.48551   ]\n",
      "  [189.33177     70.41775     81.24864   ]\n",
      "  [ 58.92569    150.86292     82.12892   ]\n",
      "  [202.14352    139.82718     92.680756  ]\n",
      "  [ 14.583278   129.50282     68.59822   ]\n",
      "  [177.0851      63.423595   216.52454   ]\n",
      "  [213.28888     23.421854    93.52908   ]\n",
      "  [170.75565    110.99343    111.9902    ]\n",
      "  [205.04758    157.03976     43.133488  ]\n",
      "  [ 93.55564    104.8012     224.26518   ]]\n",
      "\n",
      " [[ 23.478662    30.474642    62.44365   ]\n",
      "  [187.19684    153.6906      70.47193   ]\n",
      "  [136.77249    196.62975     50.404636  ]\n",
      "  [184.8448     119.012566   208.39058   ]\n",
      "  [118.354996    79.628136    37.58762   ]\n",
      "  [ 35.77324     29.037996    62.46186   ]\n",
      "  [ 30.944405   191.99748    126.60987   ]\n",
      "  [183.99637    215.25342    198.70119   ]\n",
      "  [188.59767      1.9168407   48.625954  ]\n",
      "  [109.46151     14.705184    74.869865  ]\n",
      "  [132.57303     41.40789     94.88596   ]\n",
      "  [ 91.50155    169.73357    118.79337   ]]\n",
      "\n",
      " [[109.99477     43.854977    54.25304   ]\n",
      "  [ 40.00611     12.883589   137.86519   ]\n",
      "  [139.99715     76.86699     50.310703  ]\n",
      "  [120.99627     90.5458      24.883095  ]\n",
      "  [ 73.427216   122.36814    139.02487   ]\n",
      "  [156.09406     12.39379    144.19121   ]\n",
      "  [201.53955     64.86032     52.758835  ]\n",
      "  [ 68.528564    67.9521     216.4742    ]\n",
      "  [174.22882    159.86562     85.73187   ]\n",
      "  [  5.373511   161.45425     78.65894   ]\n",
      "  [218.99911    133.9537      89.48662   ]\n",
      "  [  8.258253   131.8902      20.01062   ]]\n",
      "\n",
      " [[  6.6613197  204.17935    221.12413   ]\n",
      "  [166.22432    101.3698       8.14434   ]\n",
      "  [123.472084   196.32445    211.04524   ]\n",
      "  [ 83.333046   167.66676    175.8239    ]\n",
      "  [ 79.57025    161.44415    202.24146   ]\n",
      "  [107.286804   204.41054     46.805676  ]\n",
      "  [ 67.120674   110.74178     48.738796  ]\n",
      "  [132.75137    131.66289      8.523202  ]\n",
      "  [  1.39153     18.76224    143.66394   ]\n",
      "  [ 85.53747     72.479294   104.71105   ]\n",
      "  [184.08553    200.63399    112.49697   ]\n",
      "  [ 47.09498    216.54597    137.67027   ]]\n",
      "\n",
      " [[ 71.854935   121.337875    23.7681    ]\n",
      "  [151.49509    153.30695    195.30038   ]\n",
      "  [ 81.55707    131.07913     42.076992  ]\n",
      "  [179.36958    127.035805   135.23717   ]\n",
      "  [154.12506     21.565094   170.6605    ]\n",
      "  [133.77461    221.93799    130.34793   ]\n",
      "  [ 26.662981    25.17414    135.40143   ]\n",
      "  [ 24.33622    166.33595     21.860647  ]\n",
      "  [142.1965     106.24769    113.38293   ]\n",
      "  [ 20.560446   104.883705    22.210274  ]\n",
      "  [ 83.23949    195.85654    198.28789   ]\n",
      "  [117.21554    223.99315    147.37813   ]]\n",
      "\n",
      " [[ 42.77091     13.253787    69.32725   ]\n",
      "  [114.155914     7.435593   166.83752   ]\n",
      "  [ 92.986664    54.350807   106.80318   ]\n",
      "  [ 59.330975    42.811436    87.53872   ]\n",
      "  [ 64.5277     203.1377      63.17549   ]\n",
      "  [ 53.278435   220.42198     90.28766   ]\n",
      "  [ 72.13149     43.750156   125.68011   ]\n",
      "  [175.51431    192.16588    181.20476   ]\n",
      "  [175.0195       5.2351356  119.989265  ]\n",
      "  [  4.7047825  169.34544     29.202738  ]\n",
      "  [103.82764     65.97076      8.595863  ]\n",
      "  [ 44.533253   121.49669    135.76022   ]]]\n"
     ]
    }
   ],
   "source": [
    "input_img = tf.random_uniform(shape=[24, 24, 3], minval=0., maxval=225., seed=123, name='input_img')\n",
    "crop_input = tf.random_crop(input_img, [12, 12, 3], seed=123, name='crop')\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(input_img))\n",
    "    print('------------------------------')\n",
    "    print(sess.run(crop_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VariableV1 in module tensorflow.python.ops.variables:\n",
      "\n",
      "class VariableV1(Variable)\n",
      " |  See the [Variables Guide](https://tensorflow.org/guide/variables).\n",
      " |  \n",
      " |  A variable maintains state in the graph across calls to `run()`. You add a\n",
      " |  variable to the graph by constructing an instance of the class `Variable`.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable,\n",
      " |  which can be a `Tensor` of any type and shape. The initial value defines the\n",
      " |  type and shape of the variable. After construction, the type and shape of\n",
      " |  the variable are fixed. The value can be changed using one of the assign\n",
      " |  methods.\n",
      " |  \n",
      " |  If you want to change the shape of a variable later you have to use an\n",
      " |  `assign` Op with `validate_shape=False`.\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs for other Ops in the graph. Additionally, all the operators\n",
      " |  overloaded for the `Tensor` class are carried over to variables, so you can\n",
      " |  also add nodes to the graph by just doing arithmetic on variables.\n",
      " |  \n",
      " |  ```python\n",
      " |  import tensorflow as tf\n",
      " |  \n",
      " |  # Create a variable.\n",
      " |  w = tf.Variable(<initial-value>, name=<optional-name>)\n",
      " |  \n",
      " |  # Use the variable in the graph like any Tensor.\n",
      " |  y = tf.matmul(w, ...another variable or tensor...)\n",
      " |  \n",
      " |  # The overloaded operators are available too.\n",
      " |  z = tf.sigmoid(w + y)\n",
      " |  \n",
      " |  # Assign a new value to the variable with `assign()` or a related method.\n",
      " |  w.assign(w + 1.0)\n",
      " |  w.assign_add(1.0)\n",
      " |  ```\n",
      " |  \n",
      " |  When you launch the graph, variables have to be explicitly initialized before\n",
      " |  you can run Ops that use their value. You can initialize a variable by\n",
      " |  running its *initializer op*, restoring the variable from a save file, or\n",
      " |  simply running an `assign` Op that assigns a value to the variable. In fact,\n",
      " |  the variable *initializer op* is just an `assign` Op that assigns the\n",
      " |  variable's initial value to the variable itself.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the variable initializer.\n",
      " |      sess.run(w.initializer)\n",
      " |      # ...you now can run ops that use the value of 'w'...\n",
      " |  ```\n",
      " |  \n",
      " |  The most common initialization pattern is to use the convenience function\n",
      " |  `global_variables_initializer()` to add an Op to the graph that initializes\n",
      " |  all the variables. You then run that Op after launching the graph.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Add an Op to initialize global variables.\n",
      " |  init_op = tf.global_variables_initializer()\n",
      " |  \n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the Op that initializes global variables.\n",
      " |      sess.run(init_op)\n",
      " |      # ...you can now run any Op that uses variable values...\n",
      " |  ```\n",
      " |  \n",
      " |  If you need to create a variable with an initial value dependent on another\n",
      " |  variable, use the other variable's `initialized_value()`. This ensures that\n",
      " |  variables are initialized in the right order.\n",
      " |  \n",
      " |  All variables are automatically collected in the graph where they are\n",
      " |  created. By default, the constructor adds the new variable to the graph\n",
      " |  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n",
      " |  `global_variables()` returns the contents of that collection.\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  between variables holding the trainable model parameters and other variables\n",
      " |  such as a `global step` variable used to count training steps. To make this\n",
      " |  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n",
      " |  `True`, the new variable is also added to the graph collection\n",
      " |  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n",
      " |  `trainable_variables()` returns the contents of this collection. The\n",
      " |  various `Optimizer` classes use this collection as the default list of\n",
      " |  variables to optimize.\n",
      " |  \n",
      " |  WARNING: tf.Variable objects by default have a non-intuitive memory model. A\n",
      " |  Variable is represented internally as a mutable Tensor which can\n",
      " |  non-deterministically alias other Tensors in a graph. The set of operations\n",
      " |  which consume a Variable and can lead to aliasing is undetermined and can\n",
      " |  change across TensorFlow versions. Avoid writing code which relies on the\n",
      " |  value of a Variable either changing or not changing as other operations\n",
      " |  happen. For example, using Variable objects or simple functions thereof as\n",
      " |  predicates in a `tf.cond` is dangerous and error-prone:\n",
      " |  \n",
      " |  ```\n",
      " |  v = tf.Variable(True)\n",
      " |  tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.\n",
      " |  ```\n",
      " |  \n",
      " |  Here replacing adding `use_resource=True` when constructing the variable will\n",
      " |  fix any nondeterminism issues:\n",
      " |  ```\n",
      " |  v = tf.Variable(True, use_resource=True)\n",
      " |  tf.cond(v, lambda: v.assign(False), my_false_fn)\n",
      " |  ```\n",
      " |  \n",
      " |  To use the replacement for variables which does\n",
      " |  not have these issues:\n",
      " |  \n",
      " |  * Add `use_resource=True` when constructing `tf.Variable`;\n",
      " |  * Call `tf.get_variable_scope().set_use_resource(True)` inside a\n",
      " |    `tf.variable_scope` before the `tf.get_variable()` call.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VariableV1\n",
      " |      Variable\n",
      " |      tensorflow.python.training.checkpointable.base.CheckpointableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>)\n",
      " |      Creates a new variable with value `initial_value`.\n",
      " |      \n",
      " |      The new variable is added to the graph collections listed in `collections`,\n",
      " |      which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |      \n",
      " |      If `trainable` is `True` the variable is also added to the graph collection\n",
      " |      `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |      \n",
      " |      This constructor creates both a `variable` Op and an `assign` Op to set the\n",
      " |      variable to its initial value.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, the default, also adds the variable to the graph\n",
      " |          collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as\n",
      " |          the default list of variables to use by the `Optimizer` classes.\n",
      " |        collections: List of graph collections keys. The new variable is added to\n",
      " |          these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device.\n",
      " |          If not `None`, caches on another device.  Typical use is to cache\n",
      " |          on the device where the Ops using the Variable reside, to deduplicate\n",
      " |          copying through `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates\n",
      " |          the Variable object with its contents, referencing the variable's nodes\n",
      " |          in the graph, which must already exist. The graph is not changed.\n",
      " |          `variable_def` and the other arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type.\n",
      " |          If `None`, either the datatype will be kept (if `initial_value` is\n",
      " |          a Tensor), or `convert_to_tensor` will decide.\n",
      " |        expected_shape: A TensorShape. If set, initial_value is expected\n",
      " |          to have this shape.\n",
      " |        import_scope: Optional `string`. Name scope to add to the\n",
      " |          `Variable.` Only used when initializing from protocol buffer.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value\n",
      " |          (which must have the same shape). Constraints are not safe to\n",
      " |          use when doing asynchronous distributed training.\n",
      " |        use_resource: whether to use resource variables.\n",
      " |        synchronization: unused\n",
      " |        aggregation: unused\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Variable:\n",
      " |  \n",
      " |  __abs__ = _run_op(a, *args)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\n",
      " |          values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __div__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __floordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable. See `tf.Tensor.__getitem__` for detailed examples\n",
      " |      of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.Session() as sess:\n",
      " |        sess.run(tf.global_variables_initializer())\n",
      " |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, or Ellipsis.\n",
      " |  \n",
      " |  __gt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |  \n",
      " |  __idiv__(self, other)\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |  \n",
      " |  __invert__ = _run_op(a, *args)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __ipow__(self, other)\n",
      " |  \n",
      " |  __irealdiv__(self, other)\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |  \n",
      " |  __itruediv__(self, other)\n",
      " |  \n",
      " |  __le__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = _run_op(a, *args)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __rxor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  __sub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __xor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  assign(self, value, use_locking=False, name=None, read_value=True)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the\n",
      " |          new value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the assignment has completed.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False, name=None, read_value=True)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the\n",
      " |          new value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the addition has completed.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False, name=None, read_value=True)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: The name of the operation to be created\n",
      " |        read_value: if True, will return something which evaluates to the\n",
      " |          new value of the variable; if False will return the assign op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the subtraction has completed.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If\n",
      " |          none, the default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of Variable.shape.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable.\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See `tf.Session` for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If\n",
      " |            none, the default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Returns the value of this variable, read in the current context.\n",
      " |      \n",
      " |      Can be different from value() if it's on another device, with control\n",
      " |      dependencies, etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  scatter_add(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Adds `IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_nd_add(self, indices, updates, name=None)\n",
      " |      Applies sparse addition to individual values or slices in a Variable.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          add = ref.scatter_nd_add(indices, updates)\n",
      " |          with tf.Session() as sess:\n",
      " |            print sess.run(add)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to ref would look like this:\n",
      " |      \n",
      " |          [1, 13, 3, 14, 14, 6, 7, 20]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_nd_sub(self, indices, updates, name=None)\n",
      " |      Applies sparse subtraction to individual values or slices in a Variable.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = ref.scatter_nd_sub(indices, updates)\n",
      " |          with tf.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to ref would look like this:\n",
      " |      \n",
      " |          [1, -9, 3, -6, -6, 6, 7, -4]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_nd_update(self, indices, updates, name=None)\n",
      " |      Applies sparse assignment to individual values or slices in a Variable.\n",
      " |      \n",
      " |      `ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.\n",
      " |      \n",
      " |      `indices` must be integer tensor, containing indices into `ref`.\n",
      " |      It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\n",
      " |      \n",
      " |      The innermost dimension of `indices` (with length `K`) corresponds to\n",
      " |      indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th\n",
      " |      dimension of `ref`.\n",
      " |      \n",
      " |      `updates` is `Tensor` of rank `Q-1+P-K` with shape:\n",
      " |      \n",
      " |      ```\n",
      " |      [d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].\n",
      " |      ```\n",
      " |      \n",
      " |      For example, say we want to add 4 scattered elements to a rank-1 tensor to\n",
      " |      8 elements. In Python, that update would look like this:\n",
      " |      \n",
      " |      ```python\n",
      " |          ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n",
      " |          indices = tf.constant([[4], [3], [1] ,[7]])\n",
      " |          updates = tf.constant([9, 10, 11, 12])\n",
      " |          op = ref.scatter_nd_assign(indices, updates)\n",
      " |          with tf.Session() as sess:\n",
      " |            print sess.run(op)\n",
      " |      ```\n",
      " |      \n",
      " |      The resulting update to ref would look like this:\n",
      " |      \n",
      " |          [1, 11, 3, 10, 9, 6, 7, 12]\n",
      " |      \n",
      " |      See `tf.scatter_nd` for more details about how to make updates to\n",
      " |      slices.\n",
      " |      \n",
      " |      Args:\n",
      " |        indices: The indices to be used in the operation.\n",
      " |        updates: The values to be used in the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Subtracts `IndexedSlices` from this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  scatter_update(self, sparse_delta, use_locking=False, name=None)\n",
      " |      Assigns `IndexedSlices` to this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be assigned to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |        name: the name of the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from Variable:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Variable:\n",
      " |  \n",
      " |  constraint\n",
      " |      Returns the constraint function associated with this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The constraint function that was passed to the variable constructor.\n",
      " |        Can be `None` if no constraint was passed.\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Variable:\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.checkpointable.base.CheckpointableBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创立了张量以后，那么便可以创建相应的变量了，使用tf.Variable函数\n",
    "help(tf.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function convert_to_tensor in module tensorflow.python.framework.ops:\n",
      "\n",
      "convert_to_tensor(value, dtype=None, name=None, preferred_dtype=None)\n",
      "    Converts the given `value` to a `Tensor`.\n",
      "    \n",
      "    This function converts Python objects of various types to `Tensor`\n",
      "    objects. It accepts `Tensor` objects, numpy arrays, Python lists,\n",
      "    and Python scalars. For example:\n",
      "    \n",
      "    ```python\n",
      "    import numpy as np\n",
      "    \n",
      "    def my_func(arg):\n",
      "      arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
      "      return tf.matmul(arg, arg) + arg\n",
      "    \n",
      "    # The following calls are equivalent.\n",
      "    value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\n",
      "    value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\n",
      "    value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n",
      "    ```\n",
      "    \n",
      "    This function can be useful when composing a new operation in Python\n",
      "    (such as `my_func` in the example above). All standard Python op\n",
      "    constructors apply this function to each of their Tensor-valued\n",
      "    inputs, which allows those ops to accept numpy arrays, Python lists,\n",
      "    and scalars in addition to `Tensor` objects.\n",
      "    \n",
      "    Note: This function diverges from default Numpy behavior for `float` and\n",
      "      `string` types when `None` is present in a Python list or scalar. Rather\n",
      "      than silently converting `None` values, an error will be thrown.\n",
      "    \n",
      "    Args:\n",
      "      value: An object whose type has a registered `Tensor` conversion function.\n",
      "      dtype: Optional element type for the returned tensor. If missing, the\n",
      "        type is inferred from the type of `value`.\n",
      "      name: Optional name to use if a new `Tensor` is created.\n",
      "      preferred_dtype: Optional element type for the returned tensor,\n",
      "        used when dtype is None. In some cases, a caller may not have a\n",
      "        dtype in mind when converting to a tensor, so preferred_dtype\n",
      "        can be used as a soft preference.  If the conversion to\n",
      "        `preferred_dtype` is not possible, this argument has no effect.\n",
      "    \n",
      "    Returns:\n",
      "      An `Output` based on `value`.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If no conversion function is registered for `value`.\n",
      "      RuntimeError: If a registered conversion function returns an invalid value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.convert_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(3, 4) dtype=float32_ref>\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 创建变量通常使用的是tf.Variable，它接受的是张量输入，输出的是变量；它只是声明了变量，我们仍然需要初始化变量\n",
    "my_var = tf.Variable(tf.zeros([3, 4]))\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)  # 需要初始化\n",
    "    print(my_var)\n",
    "    print(sess.run(my_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function identity in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "identity(input, name=None)\n",
      "    Return a tensor with the same shape and contents as input.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89610445 0.7352409 ]\n",
      " [0.6590456  0.4445676 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[2, 2])\n",
    "y = tf.identity(x)\n",
    "x_val = np.random.rand(2, 2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y, feed_dict={x: x_val}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
